import os
import re
import numpy as np
from tqdm import tqdm  # –î–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
import time  # –î–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏

print(os.listdir())

filename = "–î–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π —Ç–æ–º 1-5.txt"

# –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    with open(filename, "r", encoding="utf-8") as file:
        text = file.read()
except FileNotFoundError:
    print(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    exit()

print("–ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞:")
print(text[:1000])

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
text = text.lower()
text = re.sub(r"[^\w\s]", "", text)  # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
text = re.sub(r"\d+", "", text)      # –£–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞
text = re.sub(r"\s+", " ", text).strip()
print("\n–¢–µ–∫—Å—Ç –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤):")
print(text[:1000])

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
text = text.split()
tokens = text
print(f"\n–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
word2idx = {word: idx for idx, word in enumerate(set(tokens))}
idx2word = {idx: word for word, idx in word2idx.items()}
vocab_size = len(word2idx)
print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
L = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
embedding_dims = [100, 500, 1000]  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
epochs = 3
learning_rate = 0.01
batch_size = 128  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
print("\n–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
data = []
for i in range(L, len(tokens) - L):
    context = tokens[i-L:i]
    target = tokens[i]
    data.append((context, target))

print(f"–°–æ–∑–¥–∞–Ω–æ {len(data)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
print("\n–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
for i in range(5):
    print(f"{i+1}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {data[i][0]} ‚Üí –¶–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ: {data[i][1]}")

# –§—É–Ω–∫—Ü–∏–∏ –º–æ–¥–µ–ª–∏
def initialize_weights(vocab_size, d):
    W1 = np.random.randn(vocab_size, d) * 0.01
    W2 = np.random.randn(d, vocab_size) * 0.01
    return W1, W2

def one_hot_encoding(word, vocab_size):
    vector = np.zeros(vocab_size)
    vector[word2idx[word]] = 1
    return vector

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum(axis=0)

def forward(context_words, W1, W2):
    x = np.mean(np.array([one_hot_encoding(word, vocab_size) for word in context_words]), axis=0)
    h = np.dot(W1.T, x)
    u = np.dot(W2.T, h)
    y_pred = softmax(u)
    return x, h, y_pred

def cross_entropy_loss(y_pred, y_true):
    return -np.log(y_pred[y_true] + 1e-9)

def backward(x, h, y_pred, y_true, W1, W2, learning_rate):
    y_true_one_hot = one_hot_encoding(y_true, vocab_size)
    error = y_pred - y_true_one_hot
    dW2 = np.outer(h, error)
    dW1 = np.outer(x, np.dot(W2, error))
    return dW1, dW2

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
for d in embedding_dims:
    print(f"\n{'='*50}")
    print(f"üîµ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Word2Vec (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ d = {d})")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(data)}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {epochs}")
    print(f"–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {learning_rate}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
    print(f"–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*50}\n")
    
    W1, W2 = initialize_weights(vocab_size, d)
    start_time = time.time()
    
    for epoch in range(epochs):
        epoch_loss = 0
        correct_predictions = 0
        total_examples = 0
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è —ç–ø–æ—Ö–∏
        with tqdm(data, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}", unit="–ø—Ä–∏–º–µ—Ä") as pbar:
            for i, (context, target) in enumerate(pbar):
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                x, h, y_pred = forward(context, W1, W2)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
                loss = cross_entropy_loss(y_pred, target)
                epoch_loss += loss
                
                predicted_idx = np.argmax(y_pred)
                if predicted_idx == word2idx[target]:
                    correct_predictions += 1
                total_examples += 1
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤)
                dW1, dW2 = backward(x, h, y_pred, target, W1, W2, learning_rate)
                W1 -= learning_rate * dW1
                W2 -= learning_rate * dW2
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∫–∞–∂–¥—ã–µ 100 –ø—Ä–∏–º–µ—Ä–æ–≤
                if i % 100 == 0:
                    pbar.set_postfix({
                        '–ü–æ—Ç–µ—Ä–∏': f"{epoch_loss/(i+1):.4f}",
                        '–¢–æ—á–Ω–æ—Å—Ç—å': f"{correct_predictions/(total_examples+1e-9):.2%}",
                    })
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏
        avg_loss = epoch_loss / len(data)
        accuracy = correct_predictions / len(data)
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch+1}:")
        print(f"–°—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏: {avg_loss:.4f}")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
        print(f"–í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {time.time() - start_time:.2f} —Å–µ–∫\n")
    
    total_time = time.time() - start_time
    print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É: {total_time/epochs:.2f} —Å–µ–∫")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
test_phrases = [
    ["–≤—ã—Å–æ–∫–∏–π", "—Ö—É–¥–æ–π", "–º—É–∂—á–∏–Ω–∞", "–ø–æ–¥–æ—à–µ–ª"],
    ["–∫–Ω—è–∑—å", "—Å–∫–∞–∑–∞–ª", "—á—Ç–æ", "–æ–Ω"],
    ["–æ–Ω–∞", "–ø–æ—Å–º–æ—Ç—Ä–µ–ª–∞", "–Ω–∞", "–Ω–µ–≥–æ"]
]

print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
for d in embedding_dims:
    print(f"\nüîπ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ d = {d}:")
    W1, W2 = initialize_weights(vocab_size, d)
    
    for phrase in test_phrases:
        try:
            _, _, y_pred = forward(phrase, W1, W2)
            predicted_word = idx2word[np.argmax(y_pred)]
            print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {phrase} ‚Üí –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: '{predicted_word}'")
        except KeyError as e:
            print(f"–û—à–∏–±–∫–∞: —Å–ª–æ–≤–æ {e} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ")
