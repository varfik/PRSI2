import os
import re
print(os.listdir())

filename = "–î–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π —Ç–æ–º 1-5.txt"

with open(filename, "r", encoding="utf-8") as file:
    text = file.read()

print(text[:1000])

text = text.lower()
text = re.sub(r"[^\w\s]", "", text)  # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
text = re.sub(r"\d+", "", text)      # –£–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞
text = re.sub(r"\s+", " ", text).strip()
print(text[:1000])

text = text.split()
tokens = text
print(len(text))
print(len(tokens))

# –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å "—Å–ª–æ–≤–æ -> –∏–Ω–¥–µ–∫—Å"
word2idx = {word: idx for idx, word in enumerate(set(tokens))}

# –û–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å "–∏–Ω–¥–µ–∫—Å -> —Å–ª–æ–≤–æ"
idx2word = {idx: word for word, idx in word2idx.items()}
vocab_size = len(word2idx)
print("–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è:", vocab_size)

import numpy as np

L = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

# –°–æ–∑–¥–∞—ë–º –ø–∞—Ä—ã (–∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí —Ü–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ)
data = []
for i in range(L, len(tokens) - L):
    context = tokens[i-L:i]  # L —Å–ª–æ–≤ –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ
    target = tokens[i]       # –°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ
    data.append((context, target))

# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
for i in range(30):
    print("–ö–æ–Ω—Ç–µ–∫—Å—Ç:", data[i][0], "‚Üí –¶–µ–ª–µ–≤–æ–µ —Å–ª–æ–≤–æ:", data[i][1])

import numpy as np

L = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
embedding_dims = [100, 500, 1000]  # –¢—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ d
epochs = 3
learning_rate = 0.01

# –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
vocab_size = len(word2idx)

# –§—É–Ω–∫—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
def initialize_weights(vocab_size, d):
    W1 = np.random.randn(vocab_size, d) * 0.01  # –ú–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    W2 = np.random.randn(d, vocab_size) * 0.01  # –í—ã—Ö–æ–¥–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
    return W1, W2

def one_hot_encoding(word, vocab_size):
    vector = np.zeros(vocab_size)
    vector[word2idx[word]] = 1
    return vector

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # –î–ª—è —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    return exp_x / exp_x.sum(axis=0)

def forward(context_words, W1, W2):
    # –£—Å—Ä–µ–¥–Ω—è–µ–º one-hot –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    x = np.mean(np.array([one_hot_encoding(word, vocab_size) for word in context_words]), axis=0)

    # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π (–≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
    h = np.dot(W1.T, x)

    # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π (–≤—ã—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    u = np.dot(W2.T, h)
    y_pred = softmax(u)

    return x, h, y_pred

def cross_entropy_loss(y_pred, y_true):
    return -np.log(y_pred + 1e-9)  # –î–æ–±–∞–≤–ª—è–µ–º 1e-9 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)

def backward(x, h, y_pred, y_true, W1, W2, learning_rate):
    # –ì—Ä–∞–¥–∏–µ–Ω—Ç –æ—à–∏–±–∫–∏
    y_true_one_hot = one_hot_encoding(y_true, vocab_size)
    error = y_pred - y_true_one_hot

    # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –º–∞—Ç—Ä–∏—Ü W2 –∏ W1
    dW2 = np.outer(h, error)
    dW1 = np.outer(x, np.dot(W2, error))

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
    W2 -= learning_rate * dW2
    W1 -= learning_rate * dW1

for d in embedding_dims:
    print(f"\nüîµ –û–±—É—á–∞–µ–º Word2Vec (d = {d})...\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞
    W1, W2 = initialize_weights(vocab_size, d)

    for epoch in range(epochs):
        total_loss = 0

        for context, target in data:
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            x, h, y_pred = forward(context, W1, W2)

            # –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫—É
            loss = cross_entropy_loss(y_pred, target)
            total_loss += loss

            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            #backward(x, h, y_pred, target, W1, W2, learning_rate)

        print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, –ü–æ—Ç–µ—Ä–∏: {total_loss:.4f}")

test_sentence = ["–≤—ã—Å–æ–∫–∏–π", "—Ö—É–¥–æ–π", "–º—É–∂—á–∏–Ω–∞", "–ø–æ–¥–æ—à–µ–ª"]
test_indices = [word2idx[word] for word in test_sentence]

for d in embedding_dims:
    print(f"\nüîπ d = {d}:")

    W1, W2 = initialize_weights(vocab_size, d)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞

    _, _, y_pred = forward(test_indices, W1, W2)

    predicted_word = idx2word[np.argmax(y_pred)]

    print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: {predicted_word}")